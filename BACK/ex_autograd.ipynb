{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.IntTensor(10)\n",
    "b = torch.tensor(2., requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor torch.Size([10]) 1 False None True None\n",
      "b tensor torch.Size([]) 0 True None True None\n"
     ]
    }
   ],
   "source": [
    "print('a tensor', a.shape, a.ndim, a.requires_grad, a.grad, a.is_leaf, a.grad_fn)\n",
    "print('b tensor', b.shape, b.ndim, b.requires_grad, b.grad, b.is_leaf, b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.mul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor torch.Size([10]) 1 True None False <MulBackward0 object at 0x0000028817D52340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33860\\2174180184.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\b\\abs_6fueooay2f\\croot\\pytorch-select_1707342446212\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print('c tensor', c.shape, c.ndim, c.requires_grad, c.grad, c.is_leaf, c.grad_fn)\n"
     ]
    }
   ],
   "source": [
    "print('c tensor', c.shape, c.ndim, c.requires_grad, c.grad, c.is_leaf, c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.ones(5)  # input tensor\n",
    "y1 = torch.zeros(3)  # expected tensor\n",
    "\n",
    "w1 = torch.randn(5, 3, requires_grad = True)\n",
    "b1 = torch.randn(3, requires_grad = True)\n",
    "z1 = torch.matmul(x1, w1) + b\n",
    "\n",
    "y2 = torch.zeros(1)  # expected output\n",
    "w2 = torch.randn(3, requires_grad = True)\n",
    "b2 = torch.randn(1, requires_grad = True)\n",
    "z2 = torch.matmul(z1, w2) + b2\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor torch.Size([5]) 1 False None True None\n",
      "x tensor torch.Size([3]) 1 False None True None\n",
      "w1 tensor torch.Size([5, 3]) 2 True None True None\n",
      "b1 tensor torch.Size([3]) 1 True None True None\n",
      "z1 tensor torch.Size([3]) 1 True None False <AddBackward0 object at 0x0000028817DA6970>\n",
      "w2 tensor torch.Size([3]) 1 True None True None\n",
      "b2 tensor torch.Size([1]) 1 True None True None\n",
      "z2 tensor torch.Size([1]) 1 True None False <AddBackward0 object at 0x0000028817DA6FA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33860\\2990915066.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\b\\abs_6fueooay2f\\croot\\pytorch-select_1707342446212\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print('z1 tensor', z1.shape, z1.ndim, z1.requires_grad, z1.grad, z1.is_leaf, z1.grad_fn)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33860\\2990915066.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\b\\abs_6fueooay2f\\croot\\pytorch-select_1707342446212\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print('z2 tensor', z2.shape, z2.ndim, z2.requires_grad, z2.grad, z2.is_leaf, z2.grad_fn)\n"
     ]
    }
   ],
   "source": [
    "print('x tensor', x1.shape, x1.ndim, x1.requires_grad, x1.grad, x1.is_leaf, x1.grad_fn)\n",
    "print('x tensor', y1.shape, y1.ndim, y1.requires_grad, y1.grad, y1.is_leaf, y1.grad_fn)\n",
    "\n",
    "print('w1 tensor', w1.shape, w1.ndim, w1.requires_grad, w1.grad, w1.is_leaf, w1.grad_fn)\n",
    "print('b1 tensor', b1.shape, b1.ndim, b1.requires_grad, b1.grad, b1.is_leaf, b1.grad_fn)\n",
    "print('z1 tensor', z1.shape, z1.ndim, z1.requires_grad, z1.grad, z1.is_leaf, z1.grad_fn)\n",
    "\n",
    "print('w2 tensor', w2.shape, w2.ndim, w2.requires_grad, w2.grad, w2.is_leaf, w2.grad_fn)\n",
    "print('b2 tensor', b2.shape, b2.ndim, b2.requires_grad, b2.grad, b2.is_leaf, b2.grad_fn)\n",
    "print('z2 tensor', z2.shape, z2.ndim, z2.requires_grad, z2.grad, z2.is_leaf, z2.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4161, -1.1274, -0.3324], requires_grad=True) None\n",
      "tensor([0.7247], requires_grad=True) None\n",
      "tensor([[-0.8236,  1.7063,  0.4701],\n",
      "        [ 0.8280,  0.5049, -0.4785],\n",
      "        [ 0.0085, -0.0646, -0.5571],\n",
      "        [ 2.0278,  0.7315, -1.5269],\n",
      "        [-0.4506,  0.8663, -1.1500]], requires_grad=True) None\n",
      "tensor([ 0.2837, -0.0395,  1.4867], requires_grad=True) None\n"
     ]
    }
   ],
   "source": [
    "print(w2, w2.grad)\n",
    "print(b2, b2.grad)\n",
    "print(w1, w1.grad)\n",
    "print(b1, b1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4161, -1.1274, -0.3324], requires_grad=True) tensor([ 1.0680e-04,  1.7088e-04, -3.6961e-05])\n",
      "tensor([0.7247], requires_grad=True) tensor([2.9747e-05])\n",
      "tensor([[-0.8236,  1.7063,  0.4701],\n",
      "        [ 0.8280,  0.5049, -0.4785],\n",
      "        [ 0.0085, -0.0646, -0.5571],\n",
      "        [ 2.0278,  0.7315, -1.5269],\n",
      "        [-0.4506,  0.8663, -1.1500]], requires_grad=True) tensor([[-4.2124e-05, -3.3538e-05, -9.8889e-06],\n",
      "        [-4.2124e-05, -3.3538e-05, -9.8889e-06],\n",
      "        [-4.2124e-05, -3.3538e-05, -9.8889e-06],\n",
      "        [-4.2124e-05, -3.3538e-05, -9.8889e-06],\n",
      "        [-4.2124e-05, -3.3538e-05, -9.8889e-06]])\n",
      "tensor([ 0.2837, -0.0395,  1.4867], requires_grad=True) None\n"
     ]
    }
   ],
   "source": [
    "print(w2, w2.grad)\n",
    "print(b2, b2.grad)\n",
    "print(w1, w1.grad)\n",
    "print(b1, b1.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
