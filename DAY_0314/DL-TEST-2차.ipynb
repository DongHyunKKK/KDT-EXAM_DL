{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fashion MNIST 데이터셋에서 10개 카테고리 중 하나를 선택해서 이진 분류로 구현하세요.\n",
    "- 데이터셋 : scikit-learn의 datasets 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "db_name = 'Fashion-MNIST'\n",
    "fashion_data = fetch_openml(name = db_name, parser = 'auto', as_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fashion_data.data / 255.\n",
    "y = fashion_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X_tensor[:60000], X_tensor[60000:], y_tensor[:60000], y_tensor[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)  # 5에 해당하는 옷만 True고, 다른 옷은 모두 False\n",
    "y_test_5 = (y_test == 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7252\\200613841.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_5 = torch.tensor(y_train_5, dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7252\\200613841.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_5 = torch.tensor(y_test_5, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_train_5 = torch.tensor(y_train_5, dtype = torch.float32)\n",
    "y_test_5 = torch.tensor(y_test_5, dtype = torch.float32)\n",
    "y_train_5 = y_train_5.unsqueeze(1)\n",
    "y_test_5 = y_test_5.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 1), nn.Sigmoid())\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    nb_epochs = 1000\n",
    "    for epoch in range(nb_epochs + 1):\n",
    "\n",
    "        hypothesis = model(X_train)\n",
    "\n",
    "        cost = F.binary_cross_entropy(hypothesis, y_train_5)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            prediction = hypothesis >= torch.FloatTensor([0.5])   \n",
    "            correct_prediction = prediction.float() == y_train     \n",
    "            accuracy = correct_prediction.sum().item() / len(correct_prediction) \n",
    " \n",
    "            print(f'Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f} Accuracy {accuracy * 100:2.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-1.0565e-02, -3.3298e-02, -3.2945e-02, -1.2225e-02,  3.0050e-02,\n",
      "         -8.7726e-03,  2.4070e-02,  2.9528e-02, -5.3481e-03,  2.0874e-02,\n",
      "          2.2226e-02,  8.4721e-03,  1.9678e-02,  6.3750e-04,  8.6441e-03,\n",
      "          7.6185e-03, -2.1621e-02,  4.1799e-03,  2.0110e-02,  3.0900e-03,\n",
      "          6.2568e-03,  8.3161e-03,  5.0048e-03,  2.3550e-02, -2.1457e-02,\n",
      "          1.5428e-02, -2.7021e-02,  1.0067e-02, -3.0039e-02, -7.9743e-03,\n",
      "         -9.9805e-04, -1.3779e-02,  1.6661e-02,  3.4692e-02, -2.9637e-02,\n",
      "         -2.8704e-02, -2.4241e-02, -1.0338e-02, -1.6869e-02,  1.1380e-02,\n",
      "          3.0388e-02,  2.1980e-03,  3.4724e-02, -8.7903e-04, -3.0215e-02,\n",
      "         -5.5744e-03, -1.6531e-02, -3.1952e-02,  1.0786e-02, -3.4238e-03,\n",
      "          5.5449e-03,  1.9356e-02,  2.9855e-02,  3.3290e-02,  3.5056e-03,\n",
      "         -3.1799e-02,  7.6365e-03, -1.3302e-03, -8.7480e-03,  2.8170e-03,\n",
      "         -1.7774e-02,  5.7156e-03,  1.0419e-02,  1.7763e-02,  7.8176e-03,\n",
      "          9.4850e-03, -2.5295e-02, -1.3325e-02,  2.3339e-02, -9.9649e-03,\n",
      "         -3.4174e-02,  5.8461e-05, -2.2140e-02, -2.9786e-02,  3.4250e-02,\n",
      "         -3.4723e-02,  1.2250e-02,  1.7009e-02,  3.4198e-02,  8.9026e-03,\n",
      "         -2.4884e-02,  1.7718e-02, -1.5763e-02, -3.3954e-02, -2.9041e-02,\n",
      "         -2.8266e-02, -6.7876e-03,  1.8579e-02,  3.2273e-02,  1.3164e-02,\n",
      "          3.2510e-02,  3.4439e-02,  1.7726e-02, -2.8246e-02, -4.1506e-03,\n",
      "         -3.4702e-02, -3.4659e-02,  4.8534e-04, -5.6967e-03,  3.1815e-02,\n",
      "         -3.0808e-02,  3.1685e-02,  1.7847e-02,  3.4108e-02,  1.4498e-04,\n",
      "          1.6899e-02, -3.5684e-02, -2.9377e-02,  1.3163e-02, -8.1253e-03,\n",
      "         -1.4031e-02,  3.0236e-02, -1.2231e-02,  3.4664e-02, -2.3087e-02,\n",
      "         -3.4300e-02, -2.3047e-03,  1.9998e-02,  3.5558e-02, -3.0603e-02,\n",
      "          3.0146e-02, -1.0388e-04, -2.8637e-02, -2.3303e-02, -2.1719e-03,\n",
      "         -1.0978e-02, -2.5388e-02, -3.0313e-02,  8.9478e-03, -7.9847e-03,\n",
      "          9.9192e-03,  2.6722e-02,  2.5755e-02,  2.4119e-02, -8.0538e-03,\n",
      "         -2.9932e-02, -2.6279e-02, -1.7957e-02,  2.3102e-02,  1.9549e-02,\n",
      "         -3.2570e-02,  7.9722e-03,  2.9760e-02, -7.1531e-04, -5.9914e-03,\n",
      "         -1.5846e-03,  2.9652e-02, -4.3278e-03, -7.9383e-03, -4.1116e-03,\n",
      "          1.3974e-03,  7.7378e-04,  1.7445e-02,  5.1725e-03,  2.2747e-02,\n",
      "         -2.9098e-02, -2.3040e-02,  2.7805e-02,  6.0655e-03,  6.6915e-03,\n",
      "          3.0599e-02,  3.4819e-02, -1.1162e-02,  1.4018e-02,  1.5712e-02,\n",
      "          1.2659e-02, -3.1520e-02,  2.7656e-02,  1.7002e-03,  3.4825e-03,\n",
      "          3.3090e-02, -2.1151e-02, -1.2168e-02, -3.0902e-02, -2.6628e-02,\n",
      "         -2.1730e-02,  3.3567e-02, -1.4814e-02, -3.3392e-02,  3.1431e-02,\n",
      "          2.9979e-02, -2.5567e-02, -9.7602e-04,  2.7006e-03,  2.8823e-03,\n",
      "          2.1340e-03,  1.9937e-03,  2.0457e-02,  1.2790e-02, -3.2364e-03,\n",
      "         -3.2999e-02,  8.8025e-03, -1.8987e-02, -2.1698e-02, -7.3715e-03,\n",
      "          4.2544e-03,  4.1120e-03,  2.9122e-02, -9.6647e-04, -2.5277e-02,\n",
      "         -1.1483e-02, -2.0160e-02, -1.0311e-02, -1.7068e-02, -2.6984e-02,\n",
      "          3.5013e-03, -3.4921e-02, -2.1452e-02,  2.8489e-03,  2.5144e-03,\n",
      "         -1.7830e-02, -1.8477e-02,  1.4763e-02,  2.9919e-02, -6.0591e-04,\n",
      "          3.2130e-03, -2.1294e-02,  3.6672e-03, -1.0686e-02, -3.1643e-02,\n",
      "          1.2111e-02, -1.9137e-02,  1.2183e-03,  3.5439e-02,  2.5402e-02,\n",
      "          1.4403e-02, -1.8011e-02,  3.3755e-02,  2.2069e-02, -1.7897e-02,\n",
      "         -3.4697e-05,  3.1629e-02,  3.4589e-02,  3.1064e-02, -3.5356e-02,\n",
      "          1.5905e-02,  2.1905e-03, -2.0332e-02, -1.9146e-02,  9.3152e-04,\n",
      "         -1.5795e-02,  2.8560e-02,  5.5220e-03, -3.3648e-02,  9.9942e-03,\n",
      "         -4.9139e-03,  1.8195e-02,  1.7337e-02, -5.1873e-03, -1.0755e-02,\n",
      "          3.2024e-02,  2.9126e-02, -2.2984e-02,  1.7606e-02, -2.3395e-02,\n",
      "          1.7343e-02,  3.0827e-02,  3.2854e-02, -9.4145e-04, -7.0781e-03,\n",
      "         -3.0696e-02, -1.3043e-02, -2.4634e-02,  1.0991e-02,  5.7504e-03,\n",
      "          1.9872e-03,  2.5765e-02, -2.3576e-02,  1.0057e-02,  1.3225e-02,\n",
      "         -3.4115e-02, -3.2810e-02,  5.8155e-03, -7.1080e-03, -2.5484e-02,\n",
      "         -2.6867e-02, -2.6233e-02, -2.4994e-02,  1.4787e-02,  2.6158e-02,\n",
      "         -3.9159e-04, -1.9769e-02, -2.7260e-03, -2.3815e-02,  1.2379e-02,\n",
      "          2.2085e-02,  2.8834e-02, -3.4557e-02,  3.2770e-02, -5.3788e-03,\n",
      "         -3.2284e-02, -1.6310e-02, -3.1903e-03, -3.2457e-02,  5.9254e-03,\n",
      "          3.5041e-02,  1.1788e-02, -2.3528e-02, -2.8851e-02, -2.9624e-02,\n",
      "          6.2130e-03, -7.2883e-03, -6.9436e-03,  3.0784e-03, -2.8676e-03,\n",
      "          1.8714e-02,  8.6942e-03,  3.1072e-02,  1.8453e-02,  1.1786e-02,\n",
      "         -8.3226e-03,  1.4092e-02,  3.1923e-02,  2.0941e-02, -9.3020e-03,\n",
      "         -1.0992e-02, -1.7159e-02, -1.8510e-02, -1.1703e-02,  3.1662e-02,\n",
      "         -2.6087e-02,  7.5020e-03,  2.4346e-02,  7.5933e-03,  2.1301e-02,\n",
      "          9.4912e-03,  2.5778e-02,  4.7571e-03, -7.2452e-03, -4.2409e-03,\n",
      "          8.1074e-03,  2.8187e-02,  8.2437e-04,  2.9376e-02,  1.1290e-02,\n",
      "          3.4192e-02,  1.8289e-04, -1.0598e-02,  5.3886e-03, -2.2302e-02,\n",
      "         -3.3328e-02, -1.9823e-02, -2.8943e-02,  1.8887e-02, -2.8456e-02,\n",
      "         -2.8118e-02,  1.1055e-02,  4.4772e-03, -3.2043e-02, -7.6312e-03,\n",
      "          1.5147e-02,  1.0695e-02, -3.4465e-02, -2.5326e-02,  2.2011e-02,\n",
      "          1.6324e-02,  2.4032e-03, -5.0339e-03,  4.0945e-03, -7.0972e-03,\n",
      "         -1.9144e-02,  1.8840e-02, -3.5710e-02, -2.4966e-02, -2.5264e-02,\n",
      "          2.5453e-02,  6.0704e-03,  4.5687e-03,  2.4071e-02,  1.9593e-03,\n",
      "         -8.9773e-03, -2.0493e-02, -2.0713e-02, -1.6443e-02,  5.9196e-03,\n",
      "          3.4715e-02,  2.8614e-02,  1.5672e-02, -8.5405e-03, -2.5318e-02,\n",
      "          7.4597e-03,  3.1174e-04, -1.1929e-02, -1.4403e-02, -2.2924e-03,\n",
      "          3.4719e-02,  2.6333e-02,  2.4053e-02, -2.5057e-03,  1.1778e-02,\n",
      "          5.3684e-03, -2.7178e-02, -3.4487e-02,  1.3419e-02,  1.6553e-02,\n",
      "          3.5265e-02, -1.3090e-02,  3.0047e-02,  3.1917e-02,  2.8834e-02,\n",
      "          9.1603e-03, -3.0318e-02,  1.0654e-02,  2.2759e-02, -8.0293e-03,\n",
      "         -2.6632e-02, -9.7575e-04,  2.3990e-02, -3.1941e-02,  7.2511e-03,\n",
      "         -2.9094e-02,  4.3672e-03,  2.1923e-02, -3.5188e-02,  2.2122e-02,\n",
      "         -8.5084e-03,  2.2831e-02, -2.1435e-02,  6.8314e-04, -1.7915e-02,\n",
      "          2.9069e-02,  2.8631e-03,  1.5364e-02,  2.4934e-02, -8.7903e-03,\n",
      "          2.3022e-02, -2.7737e-02, -3.1782e-02, -2.7075e-02, -2.5841e-02,\n",
      "         -2.0594e-02, -5.4976e-03, -2.4781e-02, -1.6096e-02, -1.1124e-02,\n",
      "          6.0941e-03,  2.6433e-02,  3.3028e-02,  2.4572e-02,  1.7716e-02,\n",
      "          2.5162e-02,  2.5531e-02, -3.4940e-02,  2.1485e-02, -2.6268e-02,\n",
      "          8.6623e-03, -6.6709e-04, -2.8788e-02,  5.2595e-03, -2.6621e-02,\n",
      "         -1.5374e-02, -1.0008e-02, -1.4762e-03,  1.5345e-02,  1.4791e-02,\n",
      "          3.0025e-02,  2.2231e-02, -3.0725e-02,  1.2833e-02,  3.0186e-02,\n",
      "         -1.3241e-02,  8.2762e-03,  6.6935e-03, -9.0728e-03, -2.5898e-02,\n",
      "         -2.9981e-02,  2.3936e-02,  2.6185e-02,  1.5224e-02,  1.2815e-02,\n",
      "         -1.8291e-03, -1.8045e-02, -2.6579e-02, -1.5522e-02,  1.1415e-02,\n",
      "         -1.4589e-02,  3.1026e-02,  3.5256e-02, -1.4522e-02,  4.5518e-03,\n",
      "         -2.9775e-02,  3.4675e-02,  1.1597e-02,  1.5792e-02,  6.2193e-03,\n",
      "         -1.3836e-02,  2.5214e-02,  3.2430e-02,  1.2505e-02, -9.1297e-03,\n",
      "          3.1703e-02,  9.5937e-03, -4.6843e-03,  1.9267e-02, -1.6951e-02,\n",
      "          1.0439e-02,  6.2807e-03,  1.5958e-02,  5.6573e-03,  1.7947e-02,\n",
      "          1.2406e-02,  7.7928e-03, -1.1800e-03, -3.6206e-03, -2.7444e-02,\n",
      "         -2.6077e-02,  6.8888e-04, -1.6137e-02, -2.6511e-02,  3.4199e-02,\n",
      "         -1.1232e-02,  1.1494e-02, -2.0952e-02,  2.4541e-02, -3.4121e-03,\n",
      "          3.3214e-02,  3.2114e-03,  3.3149e-02,  2.3734e-02,  9.4333e-03,\n",
      "          2.6937e-03,  1.1159e-02, -2.2332e-02, -3.0596e-02, -1.7013e-02,\n",
      "         -9.3401e-03,  2.0469e-02, -1.5752e-02,  1.5908e-02,  1.6321e-02,\n",
      "          3.4101e-02,  1.4518e-02, -1.1673e-02,  1.0557e-02,  2.8052e-02,\n",
      "          1.1448e-02,  1.6142e-02, -3.2233e-02,  2.2500e-02, -3.3981e-03,\n",
      "          3.4316e-02,  3.1214e-02,  2.3759e-02,  1.2820e-02, -2.2230e-02,\n",
      "          2.6546e-02, -1.5855e-02, -4.7511e-03, -1.2142e-02,  2.6001e-02,\n",
      "          1.3732e-02,  6.7147e-03,  3.8330e-03,  2.7848e-02,  3.3298e-02,\n",
      "         -9.5833e-03,  2.5129e-03,  3.1540e-02, -4.8903e-03, -6.4850e-03,\n",
      "          2.8806e-02, -1.9886e-02, -2.8663e-03,  1.6210e-02,  3.3796e-02,\n",
      "         -1.5669e-02, -1.7875e-02,  1.5418e-03,  7.6018e-03,  1.2418e-02,\n",
      "          1.5359e-02, -3.0258e-02, -8.7174e-03,  1.5947e-03,  8.5536e-05,\n",
      "         -1.4073e-02, -3.0838e-02,  9.8766e-03, -3.0736e-02,  5.9654e-03,\n",
      "          1.1142e-02, -3.0681e-02, -2.3046e-02, -2.0483e-02, -1.0215e-02,\n",
      "         -3.0436e-02, -4.2727e-03,  1.3650e-02,  1.7220e-02,  3.5303e-02,\n",
      "          2.7783e-02,  1.1738e-02, -1.3498e-02, -3.0302e-02,  2.7061e-02,\n",
      "          1.4710e-02,  1.5762e-02, -1.6423e-02, -3.1167e-02, -2.5410e-02,\n",
      "          1.2128e-02, -2.8400e-02,  1.4931e-02, -1.6739e-02,  3.5130e-02,\n",
      "          1.1599e-04,  4.4512e-03,  3.2978e-02,  2.4176e-02,  3.5301e-02,\n",
      "         -8.6085e-03, -2.3477e-02, -8.1026e-03,  2.1833e-02,  2.5684e-04,\n",
      "         -3.0175e-02, -1.6034e-02,  6.9500e-03, -1.2641e-03, -1.4964e-02,\n",
      "         -2.1024e-02,  2.7100e-02, -2.7600e-04,  3.5217e-02, -2.7134e-03,\n",
      "          1.4259e-02, -1.5792e-02, -3.4970e-03,  2.8977e-02,  1.6973e-02,\n",
      "         -1.3157e-02, -2.8718e-02,  1.5697e-02,  3.0632e-02,  3.0025e-02,\n",
      "         -1.9985e-02, -1.7712e-02,  3.6119e-03,  9.2516e-03,  1.2795e-02,\n",
      "          3.4719e-02,  7.2008e-03,  5.3543e-03,  6.1686e-03, -2.9107e-02,\n",
      "         -1.3956e-02, -8.1500e-03, -2.3748e-02,  1.2714e-02,  1.4740e-02,\n",
      "          2.5969e-02, -1.7782e-02,  3.4618e-02,  7.9576e-05, -2.2224e-02,\n",
      "          2.7234e-02,  1.2841e-02,  3.4264e-02,  2.5355e-02,  1.5214e-02,\n",
      "         -1.2343e-02,  2.5688e-02, -8.1887e-03, -8.2828e-03,  1.5980e-03,\n",
      "         -3.4353e-02,  3.4808e-02, -6.4279e-03, -3.3611e-02,  3.5425e-02,\n",
      "         -7.9944e-03, -1.0812e-03, -6.9223e-03,  1.3672e-02,  8.4654e-03,\n",
      "         -1.2857e-02,  3.1126e-02, -3.1053e-02,  3.4059e-02, -1.5092e-02,\n",
      "          1.2170e-02,  3.3520e-02,  2.6193e-02,  3.0201e-02, -1.1731e-02,\n",
      "         -1.4820e-03,  2.6655e-03,  2.2331e-02, -3.1918e-02, -3.0498e-03,\n",
      "         -2.8658e-02,  2.3088e-02,  3.5676e-02,  2.1931e-02, -1.0557e-02,\n",
      "          2.9611e-03, -3.3338e-02,  1.4064e-02, -2.2819e-02, -3.0660e-02,\n",
      "          2.8751e-02, -2.0186e-02,  5.6523e-03, -3.5435e-02,  3.3449e-02,\n",
      "          3.3424e-02, -1.2270e-02, -9.5254e-03,  2.9539e-02, -1.8204e-02,\n",
      "          1.5312e-02,  6.6131e-03,  1.1216e-03,  3.3883e-02,  2.3532e-02,\n",
      "         -9.4803e-03, -2.8626e-02,  2.0543e-02, -4.7685e-03,  1.8816e-02,\n",
      "          3.0277e-02,  3.3273e-02,  1.6512e-02, -2.4904e-02, -5.4998e-03,\n",
      "          3.3192e-02, -3.3855e-02, -2.8863e-02, -1.1247e-02, -8.1072e-04,\n",
      "         -1.1444e-02, -1.0755e-02,  9.1735e-03, -1.9198e-02, -2.5131e-02,\n",
      "          5.6890e-04,  1.5093e-02, -2.6278e-02,  1.5827e-02,  1.3858e-02,\n",
      "          3.4849e-02,  8.3637e-03,  3.5351e-02,  2.7667e-02,  9.5059e-03,\n",
      "          6.7167e-03, -1.1669e-02, -2.9809e-02, -2.0922e-02, -1.7944e-02,\n",
      "         -2.0242e-02,  3.5125e-02,  2.5296e-02, -7.9433e-03, -1.8724e-02,\n",
      "          3.4389e-02,  3.3378e-02, -3.2907e-03, -2.1642e-02, -3.2214e-02,\n",
      "          4.1199e-03, -2.9076e-02,  2.7180e-02,  9.2899e-03,  2.0308e-02,\n",
      "         -1.9844e-02, -9.4623e-03, -1.3005e-02,  2.4606e-02, -4.3518e-04,\n",
      "          2.2930e-02,  1.5030e-02,  4.6152e-03,  6.5119e-04, -1.8984e-02,\n",
      "          2.0602e-02,  2.7342e-02, -2.4411e-02, -1.2331e-02,  2.4881e-03,\n",
      "         -1.7906e-02,  3.2618e-02,  3.6266e-03, -5.9626e-03]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0087], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 W, b 확인\n",
    "print(list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
